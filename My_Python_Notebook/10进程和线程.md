# 进程和线程

## 简介

### 多任务

现代操作系统包括Mac OS X，UNIX，Linux，Windows等，它们都是支持“多任务”的操作系统。

什么叫“多任务”呢？简单地说，就是操作系统可以同时运行多个任务。打个比方，你一边在用浏览器上网，一边在听MP3，一边在用Word赶作业，这就是多任务，至少同时有3个任务正在运行。还有很多任务悄悄地在后台同时运行着，只是桌面上没有显示而已。

现在多核CPU已经非常普及了，使用不同的核执行不同任务自然也不是什么难事。但是，即使是过去的单核CPU，也可以执行多任务。我们知道，CPU执行代码都是顺序执行的，那么**单核CPU是怎么执行多任务的呢**？

答案就是**操作系统轮流让各个任务交替执行**，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒…… 这样反复执行下去。表面上每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们实际感觉到的就变成了所有任务都在同时执行。

真正的并行执行多任务只能在多核CPU上实现，但是，由于**任务数量往往是远多于CPU的核心数量的**，要给每个任务都分配一个CPU内核是不现实的，所以实际上操作系统会自动把很多任务轮流调度到每个核心上执行。

---

### 进程和线程的含义

对于操作系统来说，一个任务就是一个**进程（Process）**，比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。

有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把**进程内的“子任务”称为线程（Thread）**。

由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，**多个线程可以同时执行**，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正同时执行多线程需要多核CPU才可能实现。

我们前面编写的所有的Python程序，都是执行单任务的进程，并且只有一个线程。如果我们要同时执行多个任务怎么办？有三种解决方案：

- **多进程模式**：启动多个进程，每个进程只有一个线程，但多个进程可以一块执行多个任务；
- **多线程模式**：启动一个进程，在一个进程内启动多个线程，多个线程可以一块执行多个任务；
- **多进程+多线程模式**：启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。

注意，**同时执行的多个任务之间可能是关联的，需要相互通信和协调**。比方说，任务1必须先暂停等待任务2完成后才能继续执行，而任务3和任务4需要操作同一个文件所以不能同时执行。因此，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。

因为复杂度高，调试困难，所以，不是迫不得已，我们也不想编写多任务。但是，有很多时候，没有多任务还真不行。想想在电脑上看电影，就必须由一个线程播放视频，另一个线程播放音频，否则，单线程实现的话就只能先把视频播放完再播放音频，或者先把音频播放完再播放视频，这显然是不行的。

Python既支持多进程，又支持多线程，在接下来的这一章，我们会讨论如何编写这两种多任务程序。

---

### 小结

**线程是最小的执行单元，进程由至少一个线程组成**。如何调度进程和线程，完全由操作系统决定，程序自己不能决定自己什么时候被执行，执行多长时间。

多进程和多线程的程序涉及到同步、数据共享等问题，编写起来更复杂。

---

<br>

## 多进程

### fork函数

要让Python程序实现**多进程（multiprocessing）**，我们先得了解操作系统的相关知识。

Unix/Linux操作系统提供了一个 `fork()` 系统调用函数，它非常特殊。普通的函数在被调用时，调用一次只会返回一次。但是 `fork()` 函数调用一次会返回两次，因为此时操作系统会自动把当前进程（称为父进程）复制一份（称为子进程），然后分别在父进程和子进程内进行返回。

**子进程永远返回0，而父进程返回子进程的ID**。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用 `getppid()` 就可以拿到父进程的ID。

Python的 `os` 模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程：

```python
import os

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))
else:
    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))
```

运行结果如下：

```python
Process (876) start...
I (876) just created a child process (877).
I am child process (877) and my parent is 876.
```

这段代码在执行第一个print时只有一个进程（876），因此只打印一次。然后在执行 `fork()` 之后，当前进程（876）被复制出一个子进程（877）。当前进程会先返回，返回子进程ID（877），然后往下走进入if-else代码块，进入else语句进行打印；其后子进程（877）返回0，同样往下走，进入if-else代码块，进入if语句进行打印。

由于Windows没有fork调用，上面的代码在Windows上无法运行。而Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的~

有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。

---

### multiprocessing

如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？

由于Python是跨平台的，自然也应该提供跨平台的多进程支持。`multiprocessing` 模块就是跨平台版本的多进程模块。

`multiprocessing` 模块提供了一个 `Process` 类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：

```python
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
```

执行结果如下：

```python
Parent process 928.
Process will start.
Run child process test (929)...
Process end.
```

创建子进程时，只需要传入一个执行函数和该函数的参数就可以创建一个 `Process` 实例。用 `start()` 方法就可以启动这个子进程，这样创建进程要比 `fork()` 更简单和灵活。

`join()` 方法可以**等待子进程结束后再继续往下运行**，通常用于进程间的同步。

---

### 进程池

如果要启动大量的子进程，我们可以用**进程池**的方式来**批量创建子进程**：

```python
from multiprocessing import Pool
import os, time, random

# 子进程执行的任务
def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Pool(4) # 创建一个大小为4的进程池
    for i in range(5): # 依次创建5个子进程
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
```

执行结果如下：

```python
Parent process 669.
Waiting for all subprocesses done...
Run task 0 (671)...
Run task 1 (672)...
Run task 2 (673)...
Run task 3 (674)...
Task 2 runs 0.14 seconds.
Run task 4 (673)...
Task 1 runs 0.27 seconds.
Task 3 runs 0.86 seconds.
Task 0 runs 1.41 seconds.
Task 4 runs 1.91 seconds.
All subprocesses done.
```

代码解读：

对 `Pool` 对象调用 `join()` 方法会等待所有子进程执行完毕，**调用 `join()` 之前必须先调用 `close()`，调用 `close()` 之后就不能往进程池中继续添加新的 `Process` 了**。

请注意输出的结果，task 0，1，2，3这四个子进程是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为我们定义进程池时定义了大小为4，也即最多同时执行4个进程，所以第5个进程就要等进程池里有空位了才能开始。如果改成：

```python
p = Pool(5)
```

就可以同时跑5个进程。

当没有传入参数时，**Pool的默认大小是CPU的核数**，所以如果电脑是4核CPU则默认进程池大小为4，如果电脑是8核CPU则默认进程池大小为8。

---

### 子进程的输入和输出

很多时候，子进程和父进程要执行的不是同一个任务。我们创建了子进程后，还需要控制子进程的输入和输出。

`subprocess` 模块可以让我们非常方便地启动一个子进程，并且控制其输入和输出。

下面的例子演示了如何在Python代码中运行命令 `nslookup www.python.org`，这和命令行直接运行的效果是一样的：

```python
import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
```

运行结果：

```python
$ nslookup www.python.org
Server:        192.168.19.4
Address:    192.168.19.4#53

Non-authoritative answer:
www.python.org    canonical name = python.map.fastly.net.
Name:    python.map.fastly.net
Address: 199.27.79.223

Exit code: 0
```

看看帮助文档中 `subprocess` 模块的 `call()` 函数的描述：

> Run command with arguments.  Wait for command to complete or timeout, then return the returncode attribute.

它可以帮助我们建立一个子进程来执行系统调用函数，并且允许传入函数，调用后会等待运行结束并最终返回调用函数的返回值，之后才继续执行后续代码。可以只传入一个列表，列表第一个元素为系统调用的名称，第二个元素为参数。

如果子进程**执行的过程中**还需要其他输入，我们使用 `subprocess` 模块的 `Popen` 类初始化子进程，并通过它的 `communicate()` 方法来实现进程执行过程中的多次输入：

```python
import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n') # 每次输入之间用换行符隔开
print(output.decode('utf-8')) # 返回的是字节流，所以要先解码
print('Exit code:', p.returncode)
```

上面的代码相当于在命令行执行命令 `nslookup`，然后手动输入：

```python
set q=mx
python.org
exit
```

运行结果如下：

```python
$ nslookup
Server:        192.168.19.4
Address:    192.168.19.4#53

Non-authoritative answer:
python.org    mail exchanger = 50 mail.python.org.

Authoritative answers can be found from:
mail.python.org    internet address = 82.94.164.166
mail.python.org    has AAAA address 2001:888:2000:d::a6


Exit code: 0
```

---

### 进程间通信

Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的 `multiprocessing` 模块包装了底层的机制，提供了 `Queue`、`Pipes` 等多种方式来交换数据。

我们以 `Queue` 为例，在父进程中创建两个子进程，第一个子进程往 `Queue` 里写数据，第二个子进程从 `Queue` 里读数据：

```python
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
```

运行结果如下：

```python
Process to write: 50563
Put A to queue...
Process to read: 50564
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.
```

在Unix/Linux下，`multiprocessing` 模块封装了 `fork()` 调用，使我们不需要关注 `fork()` 的细节。由于Windows没有fork调用，因此，`multiprocessing` 需要“模拟”出fork的效果，**父进程的所有Python对象都必须先通过 `pickle` 序列化再传到子进程去**。所以，如果 `multiprocessing` 在Windows下调用失败了，就要先考虑是不是序列化失败了。

---

### 小结

在Unix/Linux下，可以使用 `fork()` 调用实现多进程。

要实现跨平台的多进程，可以使用 `multiprocessing` 模块。

进程间通信可以通过 `Queue`、`Pipes` 等实现的。

---

<br>

## 多线程

### 多线程简单实现

多任务可以由多进程完成，也可以由一个进程内的多线程完成。

我们前面提到了进程是由若干线程组成的，一个进程至少有一个线程。

由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。

Python的标准库提供了两个模块：`_thread` 和 `threading`，`_thread` 是低级模块，threading是高级模块，对 `_thread` 进行了封装。绝大多数情况下，我们只需要使用 `threading` 这个高级模块。

使用 `threading` 时，我们可以创建 `Thread` 实例来建立新的线程，然后调用 `start()` 启动该线程：

```python
import time, threading

# 新线程执行的代码:
def loop():
    print('thread %s is running...' % threading.current_thread().name)
    n = 0
    while n < 5:
        n = n + 1
        print('thread %s >>> %s' % (threading.current_thread().name, n))
        time.sleep(1)
    print('thread %s ended.' % threading.current_thread().name)

print('thread %s is running...' % threading.current_thread().name)
t = threading.Thread(target=loop, name='LoopThread')
t.start()
t.join()
print('thread %s ended.' % threading.current_thread().name)
```

执行结果如下：

```python
thread MainThread is running...
thread LoopThread is running...
thread LoopThread >>> 1
thread LoopThread >>> 2
thread LoopThread >>> 3
thread LoopThread >>> 4
thread LoopThread >>> 5
thread LoopThread ended.
thread MainThread ended.
```

由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的 `threading` 模块有个 `current_thread()` 函数，它永远返回当前线程的实例。**主线程实例的名字叫 `MainThread`，子线程的名字在创建时指定**，我们用 `LoopThread` 命名子线程。名字仅仅在打印时用来显示，没有其他意义，如果不起名字Python就会自动给线程命名为 `Thread-1`，`Thread-2`……

---

### 线程锁

多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响。而**多线程中，所有变量都由所有线程共享**，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程**同时改一个变量**，把内容给改乱了。

来看看多个线程同时操作一个变量怎么把内容给改乱了：

```python
import time, threading

# 假定这是你的银行存款:
balance = 0

def change_it(n):
    # 先加后减，结果应该为0:
    global balance
    balance = balance + n
    balance = balance - n

def run_thread(n):
    for i in range(100000):
        change_it(n)

t1 = threading.Thread(target=run_thread, args=(5,))
t2 = threading.Thread(target=run_thread, args=(8,))
t1.start()
t2.start()
t1.join()
t2.join()
print(balance)
```

注意，`change_it()` 函数中，使用了global关键字来声明使用的 `balance` 变量是在函数定义外部的，这样在函数内部的修改也会反映到函数外部。

`balance`，初始值为0，并且启动两个线程，先加后减，理论上结果应该为0，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，在某个时刻，线程t1和t2同时对 `balance` 变量进行修改，那么最终结果就不一定是0了。比方说：

| 时间 | 线程 | balance | 操作 |
|:-:|:-:|:-:|:-:|
| 00 | t1 | 0 | +5 |
| 01 | t1 | 5 | -5 |
| 02 | t2 | 0 | +8 |
| 03 | t2 | 8 | -8 |
| 04 | t1 | 0 | +5 |
| 05 | t2 | 5 | +8 |
| 06 | t1 | 13 | -8 |
| 07 | t2 | 5 | -5 |
| 08 | t1,t2 | 0 | +5,+8 |
| 09 | t1 | 8 | -5 |
| 10 | t2 | 3 | -8 |
| 11 | t1 | -5 | +5 |
| 12 | t2 | 0 | +8 |
| 13 | t1 | 8 | -5 |
| 14 | t2 | 3 | -8 |

假设在08时刻，t1和t2同时执行 `balance = balance + n` 这个语句，右边的 `balance` 都取0，那么t1执行完该语句时 `balance` 为5，而t2执行完时 `balance` 为8，假设t2后执行完毕，那么 `balance` 就取8，这时就产生错误了，并且错误会继续累积，当循环次数达到一定规模时，这种错误的累积会非常可怕。

为什么会产生这样的错误呢？其实 呀，这时因为高级语言的一条语句在CPU执行时需要转换为多条汇编指令，即使一个简单的计算：

```python
balance = balance + n
```

也需要分为两步：

1. 计算 `balance + n`，存入临时变量中；
2. 将临时变量的值赋给 `balance`。

可以看成：

```python
x = balance + n
balance = x
```

临时变量 `x` 是一个**局部变量**，两个线程各自都有自己的 `x`，所以当代码正常执行时：

初始值 `balance = 0`

```python
t1: x1 = balance + 5 # x1 = 0 + 5 = 5
t1: balance = x1     # balance = 5
t1: x1 = balance - 5 # x1 = 5 - 5 = 0
t1: balance = x1     # balance = 0

t2: x2 = balance + 8 # x2 = 0 + 8 = 8
t2: balance = x2     # balance = 8
t2: x2 = balance - 8 # x2 = 8 - 8 = 0
t2: balance = x2     # balance = 0
```

结果 `balance = 0`

但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2：

初始值 `balance = 0`

```python
t1: x1 = balance + 5  # x1 = 0 + 5 = 5

t2: x2 = balance + 8  # x2 = 0 + 8 = 8
t2: balance = x2      # balance = 8

t1: balance = x1      # balance = 5
t1: x1 = balance - 5  # x1 = 5 - 5 = 0
t1: balance = x1      # balance = 0

t2: x2 = balance - 8  # x2 = 0 - 8 = -8
t2: balance = x2   # balance = -8
```

结果 `balance = -8`，自然就不对了

究其原因，是因为修改 `balance` 需要多条语句，而执行这几条语句时，线程可能中断，其他线程可能也会对 `balance` 进行了修改，从而导致多个线程把同一个对象的内容改乱了。所以，我们必须确保一个线程在修改 `balance` 的时候，别的线程一定不能改。

如果我们要确保 `balance` 计算正确，就要给 `change_it()` 上一把锁，当某个线程开始执行 `change_it()` 时，我们说，该线程获得了锁，因此其他线程不能同时执行 `change_it()`，只能等待锁被释放，线程获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁可以通过 `threading.Lock()` 来实现：

```python
balance = 0
lock = threading.Lock()

def run_thread(n):
    for i in range(100000):
        # 先要获取锁:
        lock.acquire()
        try:
            # 放心地改吧:
            change_it(n)
        finally:
            # 改完了一定要释放锁:
            lock.release()
```

当多个线程同时执行 `lock.acquire()` 时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。

**获得锁的线程用完后一定要释放锁**，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们**用 `try...finally` 来确保锁一定会被释放**。

锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以构造多个不同的锁，**不同的线程持有不同的锁，在试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止**。

---

### 死循环与多核CPU

如果你拥有一个多核CPU，肯定就会想到不同的核应该可以同时执行不同的多个线程。

如果写一个死循环的话，会出现什么情况呢？

打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。我们可以监控到一个死循环线程会100%占用一个CPU。如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。

试试用Python写个死循环：

```python
import threading, multiprocessing

def loop():
    x = 0
    while True:
        x = x ^ 1

for i in range(multiprocessing.cpu_count()):
    t = threading.Thread(target=loop)
    t.start()
```

启动与CPU核心数量相同的N个线程，可以监控到CPU占用率仅有102%，也就是仅使用了一核。但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？

因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个**GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁**，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，**多线程在Python中只能交替执行**，即使100个线程跑在100核CPU上，也只能用到1个核。

GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。

所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。

不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但**可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响**。

---

### 小结

多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。

Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程并发在Python中就是一个美丽的梦。

---

<br>
